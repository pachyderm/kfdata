# Get me a Kube cluster with Kubeflow (and, for demo purposes Pachyderm) on it.

name: KFData

base:

  # NB: use:
  #     testctl ssh --tty=false -- docker images --format "{{.Repository}}:{{.Tag}}" |sort |sed 's/^/   - /g'
  # to refresh this list
  # (to get extra fancy, highlight the following lines in vim and type :! and then
  # the above command to replace it inline)
  preload_docker_images:
   - argoproj/argoui:v2.3.0
   - argoproj/workflow-controller:v2.3.0
   - awscli-alpine:latest
   - centos/mongodb-36-centos7:latest
   - gcr.io/istio-release/citadel:release-1.3-latest-daily
   - gcr.io/istio-release/galley:release-1.3-latest-daily
   - gcr.io/istio-release/kubectl:release-1.3-latest-daily
   - gcr.io/istio-release/mixer:release-1.3-latest-daily
   - gcr.io/istio-release/node-agent-k8s:release-1.3-latest-daily
   - gcr.io/istio-release/pilot:release-1.3-latest-daily
   - gcr.io/istio-release/proxy_init:release-1.3-latest-daily
   - gcr.io/istio-release/proxyv2:release-1.3-latest-daily
   - gcr.io/istio-release/sidecar_injector:release-1.3-latest-daily
   - gcr.io/k8s-minikube/storage-provisioner:v1.8.1
   - gcr.io/kfserving/kfserving-controller:v0.3.0
   - gcr.io/knative-releases/knative.dev/serving/cmd/activator:<none>
   - gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler-hpa:<none>
   - gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler:<none>
   - gcr.io/knative-releases/knative.dev/serving/cmd/controller:<none>
   - gcr.io/knative-releases/knative.dev/serving/cmd/networking/istio:<none>
   - gcr.io/knative-releases/knative.dev/serving/cmd/webhook:<none>
   - gcr.io/kubebuilder/kube-rbac-proxy:v0.4.0
   - gcr.io/kubeflow-images-public/admission-webhook:vmaster-gaf96e4e3
   - gcr.io/kubeflow-images-public/centraldashboard:vmaster-gf39279c0
   - gcr.io/kubeflow-images-public/ingress-setup:latest
   - gcr.io/kubeflow-images-public/jupyter-web-app:vmaster-gd9be4b9e
   - gcr.io/kubeflow-images-public/katib/v1alpha3/katib-controller:917164a
   - gcr.io/kubeflow-images-public/katib/v1alpha3/katib-db-manager:917164a
   - gcr.io/kubeflow-images-public/katib/v1alpha3/katib-ui:917164a
   - gcr.io/kubeflow-images-public/kfam:v1.1.0-g9f3bfd00
   - gcr.io/kubeflow-images-public/kubernetes-sigs/application:1.0-beta
   - gcr.io/kubeflow-images-public/metadata-frontend:v0.1.8
   - gcr.io/kubeflow-images-public/metadata:v0.1.11
   - gcr.io/kubeflow-images-public/notebook-controller:vmaster-gf39279c0
   - gcr.io/kubeflow-images-public/profile-controller:vmaster-g34aa47c2
   - gcr.io/kubeflow-images-public/pytorch-operator:vmaster-gd596e904
   - gcr.io/kubeflow-images-public/tf_operator:vmaster-ga2ae7bff
   - gcr.io/ml-pipeline/api-server:1.0.0
   - gcr.io/ml-pipeline/cache-deployer:1.0.0
   - gcr.io/ml-pipeline/cache-server:1.0.0
   - gcr.io/ml-pipeline/envoy:metadata-grpc
   - gcr.io/ml-pipeline/frontend:1.0.0
   - gcr.io/ml-pipeline/metadata-writer:1.0.0
   - gcr.io/ml-pipeline/minio:RELEASE.2019-08-14T20-37-41Z-license-compliance
   - gcr.io/ml-pipeline/mysql:5.6
   - gcr.io/ml-pipeline/persistenceagent:1.0.0
   - gcr.io/ml-pipeline/scheduledworkflow:1.0.0
   - gcr.io/ml-pipeline/viewer-crd-controller:1.0.0
   - gcr.io/ml-pipeline/visualization-server:1.0.0
   - gcr.io/tfx-oss-public/ml_metadata_store_server:v0.21.1
   - istio/proxyv2:1.3.1
   - k8s.gcr.io/coredns:1.6.5
   - k8s.gcr.io/etcd:3.4.3-0
   - k8s.gcr.io/kube-apiserver:v1.17.11
   - k8s.gcr.io/kube-controller-manager:v1.17.11
   - k8s.gcr.io/kube-proxy:v1.17.11
   - k8s.gcr.io/kube-scheduler:v1.17.11
   - k8s.gcr.io/pause:3.1
   - kubeflow/mxnet-operator:v1.0.0-20200625
   - metacontroller/metacontroller:v0.3.0
   - mpioperator/mpi-operator:latest
   - mysql:8
   - mysql:8.0.3
   - noobaa/noobaa-core:5.2.13
   - noobaa/noobaa-operator:2.0.10
   - pachyderm/dash:0.5.48
   - pachyderm/etcd:v3.3.5
   - pachyderm/grpc-proxy:0.4.10
   - pachyderm/pachd:1.11.0
   - prom/prometheus:v2.8.0
   - python:3.7
   - quay.io/jetstack/cert-manager-cainjector:v0.11.0
   - quay.io/jetstack/cert-manager-controller:v0.11.0
   - quay.io/jetstack/cert-manager-webhook:v0.11.0
   - quay.io/k8scsi/csi-attacher:v3.0.0-rc1
   - quay.io/k8scsi/csi-node-driver-registrar:v1.3.0
   - quay.io/k8scsi/csi-provisioner:v2.0.0-rc2
   - seldonio/seldon-core-operator:1.2.1
   - yiannisgkoufas/csi-nfs:dev-full-amd64
   - yiannisgkoufas/csi-s3:dev-full-amd64
   - yiannisgkoufas/dataset-operator:latest-amd64
   - yiannisgkoufas/generate-keys:latest-amd64

  # TODO: make this optional
  kernel_image: "quay.io/testfaster/ignite-kernel"

  # TODO: add support for dockerfile contexts to testfaster, so that we don't have to go via raw.githubusercontent.com
  # for the kfctl_k8s_istio.v1.1.0.yaml
  os_dockerfile: |-
    FROM quay.io/testfaster/kube-ubuntu

    RUN apt-get update && apt-get install -y build-essential gettext-base socat

    # kubeflow
    RUN curl -LO https://github.com/kubeflow/kfctl/releases/download/v1.1.0/kfctl_v1.1.0-0-g9a3621e_linux.tar.gz && \
        tar xf kfctl_v1.1.0-0-g9a3621e_linux.tar.gz && \
        mv kfctl /usr/local/bin

    RUN mkdir -p /workdir/yaml
    RUN wget -O /workdir/yaml/kfctl_k8s_istio.v1.1.0.yaml \
       https://raw.githubusercontent.com/pachyderm/kfdata/d851c16708cc2f18e3074aaf2a663594290b2556/kfctl_k8s_istio.v1.1.0.yaml

    RUN curl -o /tmp/pachctl.deb -L https://github.com/pachyderm/pachyderm/releases/download/v1.11.0/pachctl_1.11.0_amd64.deb && dpkg -i /tmp/pachctl.deb

  prewarm_script: |-
    #!/bin/bash
    set -euo pipefail

    # install ibm dataset lifecycle framework
    # the prewarm script is reproducible because it pins a specific commit
    git clone https://github.com/pachyderm/dataset-lifecycle-framework
    (
        cd dataset-lifecycle-framework
        git checkout 4d63924caff53b3ff0796306926422c310083410
        make deployment
    )
    (cd dataset-lifecycle-framework/examples/noobaa; ./noobaa_install.sh)

    # install pachyderm here
    pachctl deploy local

    # allow pachyderm-worker serviceaccount to create dataset-lifecycle-framework CRDs
    kubectl apply -f - <<EOF
    kind: ClusterRole
    metadata:
      name: dataset-admin
    rules:
    - apiGroups:
      - com.ie.ibm.hpsys
      resources:
      - '*'
      - datasetsinternal
      verbs:
      - '*'
    EOF

    kubectl apply -f - <<EOF
    kind: ClusterRoleBinding
    apiVersion: rbac.authorization.k8s.io/v1
    metadata:
      name: pachyderm-worker-datasets
      namespace: default
      creationTimestamp:
      labels:
        app: ''
        suite: pachyderm
    subjects:
    - kind: ServiceAccount
      name: pachyderm-worker
      namespace: default
    roleRef:
      apiGroup: ''
      kind: ClusterRole
      name: dataset-admin
    EOF

    export KF_NAME=kf

    # Set the path to the base directory where you want to store one or more
    # Kubeflow deployments. For example, /opt/.
    # Then set the Kubeflow application directory for this deployment.
    export BASE_DIR=/workdir
    export KF_DIR=${BASE_DIR}/${KF_NAME}

    # Set the configuration file to use, such as the file specified below:
    export CONFIG_URI="${BASE_DIR}/yaml/kfctl_k8s_istio.v1.1.0.yaml"
    # Generate and deploy Kubeflow:
    mkdir -p ${KF_DIR}
    cd ${KF_DIR}
    kfctl apply -V -f ${CONFIG_URI}
    cd ${BASE_DIR}

    # TODO: move the checking for unready pods into testfaster (split startup)
    unready_pods=1
    while ((unready_pods > 0)) ; do
        unready_pods=$(set +o pipefail; kubectl get po --all-namespaces --no-headers |grep -v 'Completed\|Running' |wc -l)
        echo $unready_pods unready pods
        sleep 1
    done

    echo "Got all-running pods, but waiting 10 seconds and checking again to check if we reached a stable state"
    sleep 10

    unready_pods=1
    while ((unready_pods > 0)) ; do
        unready_pods=$(set +o pipefail; kubectl get po --all-namespaces --no-headers |grep -v 'Completed\|Running' |wc -l)
        echo $unready_pods unready pods
        sleep 1
    done

    echo "Everything looks good:"
    kubectl get po --all-namespaces

    # Create anonymous namespace (profile) in Kubeflow without having to click
    # a button on a web page
    curl -XPOST http://localhost:31380/api/workgroup/create

    # disable Istio RBAC to workaround
    # https://github.com/kubeflow/pipelines/issues/4440#issuecomment-697920377
    kubectl apply -f - <<EOF
    apiVersion: rbac.istio.io/v1alpha1
    kind: ClusterRbacConfig
    metadata:
      name: default
    spec:
      mode: "OFF"
    EOF

    # tell kfp that user=anonymous@kubeflow.org even for in-cluster clients
    # like pachyderm (listenerType=ANY, not GATEWAY)
    kubectl apply -f - <<EOF
    apiVersion: networking.istio.io/v1alpha3
    kind: EnvoyFilter
    metadata:
      name: add-user-everywhere
      namespace: istio-system
    spec:
      filters:
        - filterConfig:
            inlineCode: |
              function envoy_on_request(request_handle)
                  request_handle:headers():replace("kubeflow-userid","anonymous@kubeflow.org")
              end
          filterName: envoy.lua
          filterType: HTTP
          insertPosition:
            index: FIRST
          listenerMatch:
            listenerType: SIDECAR_INBOUND
    EOF
    # Stop header being added multiple times
    kubectl delete envoyfilter -n istio-system add-user-filter


  # https://github.com/kubeflow/kubeflow/issues/5246#issuecomment-682013220
  kubernetes_version: 'v1.17.11'

runtime:
  cpus: 8
  memory: 16GB
  disk: 100GB

prewarm_pool_size: 5
max_pool_size: 15
default_lease_timeout: "" # Never timeout leases, as we use this for dev
